{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2q27gKz1H20"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TUfAcER1oUS6"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb7qyhNL1yWt"
      },
      "source": [
        "# On-device recommendation with TensorFlow Lite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw5Y7snSuG51"
      },
      "source": [
        "\u003ctable class=\"tfo-notebook-buttons\" align=\"left\"\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/lite/examples/recommendation/ml/ondevice_recommendation.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /\u003eRun in Google Colab\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/lite/examples/recommendation/ml/ondevice_recommendation.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /\u003eView source on GitHub\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "\u003c/table\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyYiyNxVp6mS"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CShg7PXmqGUJ"
      },
      "source": [
        "This code base provides a toolkit to train and serve on-device recommendation\n",
        "model. This approach personalizes recommendations by leveraging on-device data,\n",
        "and protects user privacy without having user data leave device.\n",
        "\n",
        "This Notebook shows an end-to-end example that 1) prepares sequential training data 2) trains neural-network model with various encoding techniques 3) exports the model to TensorFlow Lite 4) integrates in on-device ML applications to generate personalized recommendations.\n",
        "\n",
        "With this example, we demonstrate the approach with public\n",
        "[movielens](https://grouplens.org/datasets/movielens/) dataset, but you could\n",
        "adapt the data processing script for your dataset and train your own\n",
        "recommendation model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRBdzEu3qGFP"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m86-Nh4pMHqY"
      },
      "source": [
        "We leverage a dual-encoder model architecture, with context-encoder to encode\n",
        "sequential user history and label-encoder to encode predicted recommendation\n",
        "candidate. Similarity between context and label encodings is used to represent\n",
        "the likeliness predicted candidate meets user's needs.\n",
        "\n",
        "Three different sequential user history encoding techniques are provided with\n",
        "this code base:\n",
        "\n",
        "* Bag of words encoder (BOW): averaging user activities' embeddings without\n",
        "considering context order.\n",
        "* Convolutional neural-network encoder (CNN): applying multiple layers of\n",
        "convolutional neural-network to generate context encoding.\n",
        "* Recurrent neural-network encoder (RNN): applying recurrent neural network\n",
        "(LSTM in this example) to understand context sequence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcLF2PKkSbV3"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "To run this example, please clone the source code from github [repo](https://github.com/tensorflow/examples/tree/master/lite/examples/recommendation/ml), install required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cv3K3oaksJv"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/tensorflow/examples\n",
        "!cd examples/lite/examples/recommendation/ml/\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BOGJnbVtCQQ"
      },
      "source": [
        "## Training data preparation\n",
        "\n",
        "This notebook makes use of public dataset [movielens](https://grouplens.org/datasets/movielens/) as training data for on-device movie recommendations. The data processing script performs the following steps:\n",
        "\n",
        "\n",
        "*   Downloads [movielens](https://grouplens.org/datasets/movielens/) dataset\n",
        "*   Groups movie rating records by user, and orders per-user movie rating records by timestamp.\n",
        "*   Generates Tensorflow examples with features: 1) \"context\": time-ordered sequential movie IDs 2) \"label\": next movie ID user rated as label. \"max_history_length\" is taken in as parameter to define \"context\" feature shape, if not enough history found, right padding with out-of-vocab ID 0 will be performed.\n",
        "\n",
        "\n",
        "Note: If you would like to use your own data, please adapt the data processing script for your specific case.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQvryCfGtCQX"
      },
      "outputs": [],
      "source": [
        "!python -m data.example_generation_movielens \\\n",
        "  --data_dir=data/raw \\\n",
        "  --output_dir=data/examples \\\n",
        "  --build_movie_vocab=True \\\n",
        "  --min_timeline_length=3 \\\n",
        "  --max_context_length=10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zcEXFkgCz8g"
      },
      "source": [
        "Raw movielens ratings.dat data is in the following format:\n",
        "UserID::MovieID::Rating::Timestamp\n",
        "\n",
        "*   UserIDs range between 1 and 6040\n",
        "*   MovieIDs range between 1 and 3952\n",
        "*   Ratings are made on a 5-star scale (whole-star ratings only)\n",
        "*   Timestamp is represented in seconds since the epoch as returned by time(2)\n",
        "*   Each user has at least 20 ratings\n",
        "\n",
        "Ref:[movielens readme.txt](http://files.grouplens.org/datasets/movielens/ml-1m-README.txt)\n",
        "\n",
        "In this example, we consider each rating as a movie watch by the users, and construct user movie watch history with rated movie IDs ordering by time.\n",
        "\n",
        "Sample generated training example with max user history as 10:\n",
        "```\n",
        "0 : {   # (tensorflow.Example)\n",
        "  features: {   # (tensorflow.Features)\n",
        "    feature: {\n",
        "      key  : \"context\"\n",
        "      value: {\n",
        "        int64_list: {\n",
        "          value: [ 595, 2687, 745, 588, 1, 2355, 2294, 783, 1566, 1907 ]\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    feature: {\n",
        "      key  : \"label\"\n",
        "      value: {\n",
        "        int64_list: {\n",
        "          value: [ 48 ]\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQcQ6AssuBN8"
      },
      "source": [
        "# Train model\n",
        "\n",
        "The training launcher script uses TensorFlow keras compile/fit APIs and performs\n",
        "the following steps to kick start training and evaluation process:\n",
        "\n",
        "*   Set up both train and eval dataset input function.\n",
        "*   Construct keras model according to provided configs, please refer to sample.config file in the source code to config your model architecture, such as embedding dimension, convolutional neural network params, LSTM units etc.\n",
        "*   Setup loss function. In this code base, we leverages customized batch softmax loss function.\n",
        "*   Setup optimizer, with flag specified learning rate and gradient clip if needed.\n",
        "*   Setup evaluation metrics, we provided recall@k metrics by default.\n",
        "*   Compile model with loss function, optimizer and defined metrics.\n",
        "*   Setup callbacks for tensorboard and checkpoint manager.\n",
        "*   Run model.fit with compiled model, where you could specify number of epochs to train, number of train steps in each epoch and number of eval steps in each epoch.\n",
        "\n",
        "To start training please execute command:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gPKz5InxEbF"
      },
      "outputs": [],
      "source": [
        "!python -m model.recommendation_model_launcher_keras \\\n",
        "  --run_mode \"train_and_eval\" \\\n",
        "  --encoder_type \"bow\" \\\n",
        "  --training_data_filepattern \"data/examples/train_movielens_1m.tfrecord\" \\\n",
        "  --testing_data_filepattern \"data/examples/test_movielens_1m.tfrecord\" \\\n",
        "  --model_dir \"model/model_dir\" \\\n",
        "  --params_path \"model/sample_config.json\"\\\n",
        "  --batch_size 16 \\\n",
        "  --learning_rate 0.01 \\\n",
        "  --steps_per_epoch 1000 \\\n",
        "  --num_epochs 10000 \\\n",
        "  --num_eval_steps 1000 \\\n",
        "  --gradient_clip_norm 1.0 \\\n",
        "  --max_history_length 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObH_mcGcxS96"
      },
      "source": [
        "# Export model\n",
        "\n",
        "Inside launcher script we also provide model exportation functionality.\n",
        "\n",
        "In serve model, the model takes in user context history, for the example case the input is a vector of movie IDs you interacted with. With context encoder, model computes the context embedding vector, at the same time generate candidate embedding vector for all movie candidates in the vocab. By dotproduct and top-k ranking, top-k candidates will be served as the predicted candidates.\n",
        "\n",
        "At model exportation step, you could specify number of predictions you want to get from the output of the model.\n",
        "\n",
        "This step includes:\n",
        "\n",
        "\n",
        "*   Export the model to saved_model with tf.saved_model.save.\n",
        "*   Convert the saved_model to TensorFlow lite with tf.lite.TFLiteConverter.from_saved_model, and save it to the export directory wanted.\n",
        "\n",
        "To export the model, please execute command:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SH5r6AxHzGrS"
      },
      "outputs": [],
      "source": [
        "!python -m model.recommendation_model_launcher_keras \\\n",
        "  --run_mode \"export\" \\\n",
        "  --params_path \"model/sample_config.json\"\\\n",
        "  --model_dir \"model/model_dir\" \\\n",
        "  --checkpoint_path \"model/model_dir/ckpt-1000\" \\\n",
        "  --num_predictions 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXMQ5D5JzSgv"
      },
      "source": [
        "# Model inference\n",
        "\n",
        "You could verify your model's performance by running inference with test examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "og0qkYavz3Nt"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Use [0, 1, ... 9] as example input to represent 10 movies that user interacted with.\n",
        "context = tf.range(10)\n",
        "# Directory to exported TensorFlow Lite model.\n",
        "export_dir = \"\"\n",
        "tflite_model_path = os.path.join(export_dir, 'model.tflite')\n",
        "f = open(tflite_model_path, 'rb')\n",
        "interpreter = tf.lite.Interpreter(model_content=f.read())\n",
        "interpreter.allocate_tensors()\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "print(input_details)\n",
        "print(output_details)\n",
        "\n",
        "interpreter.set_tensor(input_details[0]['index'], context)\n",
        "interpreter.invoke()\n",
        "tflite_top_predictions_ids = interpreter.get_tensor(\n",
        "    output_details[0]['index'])\n",
        "tflite_top_prediction_scores = interpreter.get_tensor(\n",
        "    output_details[1]['index'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_omMjoT035u"
      },
      "source": [
        "# Integrate in your application\n",
        "\n",
        "We also open source an Android reference app to run inference with TF Lite.\n",
        "**Please follow [`android/app/README.md`](https://github.com/tensorflow/examples/blob/master/lite/examples/recommendation/android/README.md)** to install required developer tools and build Android app."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N83Ev6nSwsUW"
      },
      "source": [
        "The app uses one pretrained model to illustrate how to run TFLite. If you want to replace the existing model with the one you just trained above, please copy the respective TF Lite model to `assets` folder, and adapt its file name accordingly. If you directly train and export your model in this notebook, your\n",
        "exported model should be located at \"model/model_dir/export/model.tflite\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUaXjqGBvnFP"
      },
      "source": [
        "```shell\n",
        "cp path/to/your/model.tflite ../android/app/src/main/assets/\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwr7pigOB7pT"
      },
      "source": [
        "The app uses the json file `config.json` to load one model and control how to consume IDs and scores predicted by the TF Lite recommendation model on device. `Config` definition can be found in [`android/app/src/main/java/org/tensorflow/lite/examples/recommendation/Config.java`](../android/app/src/main/java/org/tensorflow/lite/examples/recommendation/Config.java).\n",
        "\n",
        "A sample json is presented below for the built-in model, and you may need to *adapt* it for your own trained model.\n",
        "\n",
        "``` json\n",
        "{\n",
        "  \"model\": \"model_history10_top100.tflite\",\n",
        "  \"inputLength\": 10,\n",
        "  \"outputLength\": 100,\n",
        "  \"topK\": 10,\n",
        "  \"movieList\": \"sorted_movie_vocab.json\"\n",
        "}\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ondevice_recommendation.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
